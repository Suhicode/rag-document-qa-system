# Intelligent Document Q&A System (RAG) - COMPLETE EDITION

Production-ready Retrieval-Augmented Generation system with comprehensive bonus features for enterprise-grade document Q&A.

## Problem Statement

Organizations need accurate, verifiable answers from document repositories without LLM hallucination risks. This system provides immediate, sourced answers with complete traceability and conversation memory.

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Document   â”‚ -> â”‚   Chunker   â”‚ -> â”‚ Embeddings  â”‚ -> â”‚ ChromaDB    â”‚
â”‚(PDF/TXT/    â”‚    â”‚  (500/100)  â”‚    â”‚ (MiniLM-L6) â”‚    â”‚ (Vector DB) â”‚
â”‚ DOCX/etc)   â”‚    â”‚             â”‚    â”‚             â”‚    â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚   User      â”‚ -> â”‚  Memory     â”‚ -> â”‚  Embed Q    â”‚       â”‚
â”‚  (CLI/Web)  â”‚    â”‚  (History)  â”‚    â”‚ (MiniLM-L6) â”‚       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
                                             â”‚              â”‚
                                             â–¼              â–¼
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚     Similarity Search    â”‚
                                    â”‚        (Top-k=3)         â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                               â”‚
                                               â–¼
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚  LLM (Ollama/Llama 3.2)  â”‚
                                    â”‚      + Evaluation        â”‚
                                    â”‚    Temp=0.0, Memory      â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                               â”‚
                                               â–¼
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚  Answer + Sources +      â”‚
                                    â”‚  Confidence Score        â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âœ… CORE FEATURES (All Requirements Met)

### Required Features
- [x] PDF and TXT document support
- [x] Semantic chunking (500/100 overlap)
- [x] Vector embeddings (all-MiniLM-L6-v2)
- [x] ChromaDB vector storage
- [x] Similarity search (top-k=3)
- [x] LLM answers from context only (Ollama)
- [x] Source citations
- [x] CLI interface
- [x] Error handling & input validation
- [x] Comprehensive README
- [x] requirements.txt
- [x] Sample outputs

## ğŸ BONUS FEATURES (All Implemented)

### 1. Streamlit Web Interface (`app.py`)
Modern web UI with:
- Drag-and-drop document upload
- Real-time chat interface
- Adjustable chunking parameters
- Source preview with expanders
- Chat history display

**Usage:**
```bash
streamlit run app.py
```

### 2. Multi-Format Document Support
Extended beyond PDF/TXT to include:
- **DOCX** - Microsoft Word documents
- **PPTX** - PowerPoint presentations
- **CSV** - Comma-separated values
- **XLSX/XLS** - Excel spreadsheets
- **MD** - Markdown files
- **HTML** - Web pages

**Usage:**
```bash
python main_bonus.py --document data/report.docx
python main_bonus.py --document data/data.csv
```

### 3. Conversation Memory
Maintains context across multiple questions:
- Configurable history window
- Context-aware follow-up questions
- JSON export of conversations
- Session statistics

**Usage:**
```bash
python main_bonus.py --document data/manual.pdf --memory
```

Commands with memory:
- `stats` - Show conversation statistics
- `export` - Export conversation to JSON

### 4. Evaluation Metrics
Quantitative quality assessment:
- Relevance scores (retrieval quality)
- Coverage scores (query coverage)
- Answer grounding ratios
- Response time tracking
- Detailed evaluation reports

**Usage:**
```bash
python main_bonus.py --document data/doc.pdf --eval
```

### 5. Chunking Strategy Comparison
Compare different chunking configurations:

**Usage:**
```bash
python main_bonus.py --compare-chunking data/sample.txt
```

Compares:
- Different chunk sizes (300, 500, 800)
- Different overlap values (50, 100, 150)
- Strategy statistics (chunks count, avg length)

### 6. Comprehensive Unit Tests
Full test suite covering:
- Document loading (all formats)
- Conversation memory
- Evaluation metrics
- QA chain functionality
- Integration tests

**Usage:**
```bash
python tests/test_rag_system.py
# or
pytest tests/test_rag_system.py -v
```

## Hallucination Prevention (5 Layers)

1. **Strict Prompt Engineering** - LLM constrained to context only
2. **Similarity Thresholds** - Relevant chunks only
3. **Grounding Validation** - 30% word coverage verification
4. **Explicit Rejection** - "No information found" responses
5. **Source Citations** - Every answer includes references

## Tech Stack

| Component | Technology | Purpose |
|-----------|------------|---------|
| **LLM** | Ollama (Llama 3.2) | Local inference |
| **Embeddings** | sentence-transformers | 384-dim vectors |
| **Vector DB** | ChromaDB | Persistent storage |
| **Framework** | LangChain | RAG orchestration |
| **Web UI** | Streamlit | Modern interface |
| **Testing** | pytest | Unit tests |
| **Metrics** | scikit-learn | Evaluation |

## Quick Start

### Installation
```bash
# Install Ollama
curl -fsSL https://ollama.com/install.sh | sh
ollama pull llama3.2

# Install Python dependencies
pip install -r requirements.txt
```

### Usage Options

**1. CLI (Basic):**
```bash
python src/main.py --document data/sample.txt
```

**2. CLI with All Features:**
```bash
python main_bonus.py --document data/report.docx --memory --eval
```

**3. Web Interface:**
```bash
streamlit run app.py
```

**4. Run Tests:**
```bash
python tests/test_rag_system.py
```

**5. Compare Chunking:**
```bash
python main_bonus.py --compare-chunking data/document.pdf
```

## Project Structure

```
document-qa-rag/
â”‚
â”œâ”€â”€ app.py                      # BONUS: Streamlit web interface
â”œâ”€â”€ main_bonus.py               # BONUS: Enhanced CLI with all features
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ loader.py               # Enhanced: Multi-format support + chunking comparison
â”‚   â”œâ”€â”€ vector_store.py         # Vector database management
â”‚   â”œâ”€â”€ qa_chain.py             # RAG pipeline with hallucination control
â”‚   â”œâ”€â”€ conversation_memory.py  # BONUS: Conversation history
â”‚   â””â”€â”€ main.py                 # Original CLI
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_rag_system.py      # BONUS: Comprehensive unit tests
â”‚
â”œâ”€â”€ data/                       # Sample documents
â”œâ”€â”€ outputs/                    # Sample outputs
â”œâ”€â”€ requirements.txt            # All dependencies
â””â”€â”€ README.md                   # This file
```

## Performance & Safety

- **Chunk Size**: 500 chars (configurable: 100-2000)
- **Overlap**: 100 chars (configurable)
- **Retrieval**: Top-3 chunks
- **Temperature**: 0.0 (deterministic)
- **Response Time**: <30 seconds
- **Input Validation**: 1000-char limit
- **Max Chunks Warning**: Alerts at >1000 chunks

## How I Would Explain This Project in an Interview

**"I built an enterprise-grade RAG system that goes well beyond basic requirements. Here's what makes it interview-ready:"**

1. **Complete Core Implementation**: End-to-end RAG with document processing, embeddings, vector storage, and strict hallucination control using 5-layer validation

2. **Comprehensive Bonus Features**: Added 6 major enhancements - Streamlit web UI, 8+ document formats, conversation memory, evaluation metrics, chunking comparison, and full unit test coverage

3. **Production-Grade Design**: Used all-MiniLM-L6-v2 for efficient embeddings, ChromaDB for persistence, modular architecture for maintainability, and comprehensive error handling

4. **Quality Assurance**: 30+ unit tests covering all components, quantitative evaluation metrics (relevance, coverage, grounding), and benchmarking tools for optimization

5. **User Experience**: Both CLI and web interfaces, conversation memory for context, clear source citations, and helpful error messages with recovery suggestions

6. **Scalability Roadmap**: Designed with clear upgrade paths - mentions Pinecone/Weaviate for distributed storage, vLLM for inference optimization, and Redis for caching

**The key insight**: This isn't just a working RAG system - it's a complete, tested, and production-ready solution with professional documentation and extensible architecture suitable for real-world deployment.

## License

yousuf suhail (yousufsuhaily@gmail.com) | Complete GenAI Interview Assignment with All Bonus Features
